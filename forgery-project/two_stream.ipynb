{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4lz8VDKved3D",
    "outputId": "182c58c2-c028-45ef-c29f-eaf1c208d4a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (3.4.2)\n",
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.13.0 typeguard-2.12.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vdJ2fWiPed3K"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization, Lambda, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input\n",
    "# import tf_slim as slim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "%matplotlib inline\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0UnFDto_ed3L"
   },
   "outputs": [],
   "source": [
    "def initializer():\n",
    "    filter1 = [[0, 0, 0, 0, 0],\n",
    "               [0, -1, 2, -1, 0],\n",
    "               [0, 2, -4, 2, 0],\n",
    "               [0, -1, 2, -1, 0],\n",
    "               [0, 0, 0, 0, 0]]\n",
    "    filter2 = [[-1, 2, -2, 2, -1],\n",
    "               [2, -6, 8, -6, 2],\n",
    "               [-2, 8, -12, 8, -2],\n",
    "               [2, -6, 8, -6, 2],\n",
    "               [-1, 2, -2, 2, -1]]\n",
    "    filter3 = [[0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [0, 1, -2, 1, 0],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0]]\n",
    "    q = [4.0, 12.0, 2.0]\n",
    "    filter1 = np.asarray(filter1, dtype=float) / 4\n",
    "    filter2 = np.asarray(filter2, dtype=float) / 12\n",
    "    filter3 = np.asarray(filter3, dtype=float) / 2\n",
    "    filters = [[filter1, filter1, filter1], [filter2, filter2, filter2], [filter3, filter3, filter3]]\n",
    "    filters = np.einsum('klij->ijlk', filters)\n",
    "    filters = tf.Variable(filters, dtype=tf.float32)\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z3G7HUolf9S0"
   },
   "outputs": [],
   "source": [
    "def convert_to_ela_image(path, quality):\n",
    "    temp_filename = 'temp_file_name.jpg'\n",
    "    ela_filename = 'temp_ela.png'\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image.save(temp_filename, 'JPEG', quality = quality)\n",
    "    temp_image = Image.open(temp_filename)\n",
    "    \n",
    "    ela_image = ImageChops.difference(image, temp_image)\n",
    "    \n",
    "    extrema = ela_image.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    \n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "    \n",
    "    return ela_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QzNcv1sied3L"
   },
   "outputs": [],
   "source": [
    "h = 224\n",
    "w = 224\n",
    "image_size = (h, w)\n",
    "\n",
    "# np_arr = lambda img: img.resize(image_size).flatten() / 255.0\n",
    "np_arr = lambda img: np.array(img.resize(image_size)).flatten() / 255.0\n",
    "\n",
    "def prepare_image(image_path, is_ela):\n",
    "    return np_arr(convert_to_ela_image(image_path, 95)) if is_ela == 1 else np_arr(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OG4Y6Djyed3L"
   },
   "outputs": [],
   "source": [
    "x_srm = [] # SRM converted images\n",
    "x_ela = [] # ELA converted images\n",
    "labels = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VZnqIkRred3M"
   },
   "outputs": [],
   "source": [
    "def prepare_data(path, cls, srm, ela, targets):\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('jpg') or filename.endswith('png'):\n",
    "                try:\n",
    "                    full_path = os.path.join(dirname, filename)\n",
    "                    srm.append(prepare_image(full_path, 0))\n",
    "                    ela.append(prepare_image(full_path, 1))\n",
    "                    targets.append(cls)\n",
    "                except:\n",
    "                    pass\n",
    "                if len(targets) % 500 == 0:\n",
    "                    print('Processing {} images'.format(len(targets)))\n",
    "    print(len(srm), len(ela), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0nv-AJSfed3M",
    "outputId": "555d4c5a-b5e2-43a7-e861-87b2ce4a3de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 images\n",
      "Processing 1000 images\n",
      "Processing 1500 images\n",
      "Processing 2000 images\n",
      "Processing 2500 images\n",
      "Processing 3000 images\n",
      "Processing 3500 images\n",
      "Processing 4000 images\n",
      "Processing 4500 images\n",
      "Processing 5000 images\n",
      "Processing 5500 images\n",
      "Processing 6000 images\n",
      "Processing 6500 images\n",
      "Processing 7000 images\n",
      "Processing 7500 images\n",
      "Processing 8000 images\n",
      "8173 8173 8173\n"
     ]
    }
   ],
   "source": [
    "#place authentic\n",
    "Au_path = '../synthetic/Au'\n",
    "prepare_data(Au_path, 1, x_srm, x_ela, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JCDWFw6Oed3M",
    "outputId": "e35933dd-1ce3-4549-a001-8c8b29373a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8500 images\n",
      "Processing 9000 images\n",
      "Processing 9500 images\n",
      "Processing 10000 images\n",
      "Processing 10500 images\n",
      "Processing 11000 images\n",
      "Processing 11500 images\n",
      "Processing 12000 images\n",
      "12477 12477 12477\n"
     ]
    }
   ],
   "source": [
    "#place tampered\n",
    "Tp_path = '../synthetic/Tp'\n",
    "prepare_data(Tp_path, 0, x_srm, x_ela, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaping(srm, ela, targets):\n",
    "    srm = np.array(srm)\n",
    "    ela = np.array(ela)\n",
    "\n",
    "    targets = to_categorical(targets, 2)\n",
    "\n",
    "    srm = srm.reshape(-1, h, w, 3)\n",
    "    ela = ela.reshape(-1, h, w, 3)\n",
    "\n",
    "    print(srm.shape, ela.shape, targets.shape)\n",
    "    \n",
    "    # stack so we can split on the same pair of images\n",
    "    x_combined = np.stack((srm, ela), axis=4)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_combined, targets, test_size = 0.2, random_state=5)\n",
    "\n",
    "    # take them apart\n",
    "    x_train_srm = x_train[:,:,:,:,0]\n",
    "    x_val_srm = x_val[:,:,:,:,0]\n",
    "\n",
    "    x_train_ela = x_train[:,:,:,:,1]\n",
    "    x_val_ela = x_val[:,:,:,:,1]\n",
    "    \n",
    "    return x_train_srm, x_val_srm, x_train_ela, x_val_ela, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12477, 224, 224, 3) (12477, 224, 224, 3) (12477, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train_srm, x_val_srm, x_train_ela, x_val_ela, y_train, y_val = reshaping(x_srm, x_ela, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wyqdzWMGed3N"
   },
   "outputs": [],
   "source": [
    "initializer_srm = initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_product(x):\n",
    "    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
    "    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n",
    "    \n",
    "    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
    "    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n",
    "    \n",
    "    # Divide by feature map size [sizexsize]\n",
    "    size1 = int(x[1].shape[1])\n",
    "    size2 = int(x[1].shape[2])\n",
    "    phi_I = tf.divide(phi_I, size1*size2)\n",
    "    \n",
    "    # Take signed square root of phi_I\n",
    "    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n",
    "    \n",
    "    # Apply l2 normalization\n",
    "    z_l2 = tf.math.l2_normalize(y_ssqrt, axis=1)\n",
    "    return z_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P7iZvzfEkuIx"
   },
   "outputs": [],
   "source": [
    "def conv_layers(input):\n",
    "    x = Conv2D(32, 3, padding='valid', activation='relu')(input)\n",
    "    x = Conv2D(32, 3, padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(64, 3, padding='valid', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, 3, padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, 5, padding='valid', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 5, padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "#     x = Conv2D(256, 5, padding='valid', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Conv2D(256, 5, padding='valid', activation='relu')(x)\n",
    "#     x = MaxPool2D(pool_size=2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "#   x = Conv2D(512, 3, padding='valid', activation='relu')(x)\n",
    "#   x = BatchNormalization()(x)\n",
    "#   x = Conv2D(512, 3, padding='valid', activation='relu')(x)\n",
    "#   x = MaxPool2D(pool_size=2)(x)\n",
    "#   model = BatchNormalization()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layers_2(input):\n",
    "    x = Conv2D(32, 3, padding='same')(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(32, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(64, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(256, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6ptBj_9MkVvF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-26 21:52:13.405 tensorflow-2-3-gp-ml-g4dn-12xlarge-1be1fa3f01e6a5645cff1d084b39:51 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-05-26 21:52:13.430 tensorflow-2-3-gp-ml-g4dn-12xlarge-1be1fa3f01e6a5645cff1d084b39:51 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "srm_input (InputLayer)          [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_srm-layer (TensorFl [(None, 224, 224, 3) 0           srm_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ela_input (InputLayer)          [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 222, 222, 32) 896         tf_op_layer_srm-layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 222, 222, 32) 896         ela_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 220, 220, 32) 9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 220, 220, 32) 9248        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 110, 110, 32) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 110, 110, 32) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 110, 110, 32) 128         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 110, 110, 32) 128         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 108, 108, 64) 18496       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 108, 108, 64) 18496       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 108, 108, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 108, 108, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 106, 106, 64) 36928       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 106, 106, 64) 36928       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 53, 53, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 53, 53, 64)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 53, 53, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 53, 53, 64)   256         max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 49, 49, 128)  204928      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 49, 49, 128)  204928      batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 49, 49, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 49, 49, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 45, 45, 128)  409728      batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 45, 45, 128)  409728      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 22, 22, 128)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 22, 22, 128)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 22, 22, 128)  512         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 22, 22, 128)  512         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 18, 18, 256)  819456      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 18, 18, 256)  819456      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 18, 18, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 18, 18, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 14, 14, 256)  1638656     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 14, 256)  1638656     batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 256)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 7, 7, 256)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 7, 7, 256)    1024        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 7, 7, 256)    1024        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outer_product (Lambda)          (None, 65536)        0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 65536)        0           outer_product[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          16777472    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256)          1024        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            514         batch_normalization_14[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 23,063,106\n",
      "Trainable params: 23,058,882\n",
      "Non-trainable params: 4,224\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# SRM stream\n",
    "srm_input = Input(shape=[h, w, 3], name='srm_input')\n",
    "op = tf.nn.conv2d(srm_input, initializer_srm, strides=[1, 1, 1, 1], padding='SAME', name='srm-layer')\n",
    "srm_model = conv_layers(op)\n",
    "\n",
    "# ELA stream\n",
    "ela_input = Input(shape=[h, w, 3], name='ela_input')\n",
    "ela_model = conv_layers(ela_input)\n",
    "\n",
    "# Concatenate streams\n",
    "# concat = tf.keras.layers.Concatenate()([srm_model, ela_model])\n",
    "\n",
    "# Bilinear fusion\n",
    "x = Lambda(outer_product, name='outer_product')([srm_model,ela_model])\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "# x = Dense(256)(x)\n",
    "# x = LeakyReLU(alpha=0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[srm_input, ela_input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iV1_nNvued3O"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "init_lr = 1e-4\n",
    "# optimizer = Adam(lr = init_lr)\n",
    "optimizer = Adam(learning_rate = init_lr, amsgrad=True)\n",
    "# decay = init_lr/epochs\n",
    "# optimizer = RMSprop(learning_rate = init_lr, centered = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0,patience=5, verbose=0, mode='auto')\n",
    "\n",
    "checkpoint_filepath = 'two_stream/checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "# model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOn-5ptued3O",
    "outputId": "f3b32633-a783-4093-f716-8309c7e5fdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/312 [..............................] - ETA: 38s - loss: 0.6353 - accuracy: 0.7656WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0817s vs `on_train_batch_end` time: 0.1692s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0817s vs `on_train_batch_end` time: 0.1692s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.8557WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0123s vs `on_test_batch_end` time: 0.0711s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0123s vs `on_test_batch_end` time: 0.0711s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 92s 294ms/step - loss: 0.3885 - accuracy: 0.8557 - val_loss: 0.6846 - val_accuracy: 0.5469\n",
      "Epoch 2/50\n",
      "312/312 [==============================] - 87s 280ms/step - loss: 0.3680 - accuracy: 0.8698 - val_loss: 0.4188 - val_accuracy: 0.8325\n",
      "Epoch 3/50\n",
      "312/312 [==============================] - 87s 279ms/step - loss: 0.3501 - accuracy: 0.8774 - val_loss: 0.3469 - val_accuracy: 0.8730\n",
      "Epoch 4/50\n",
      "312/312 [==============================] - 88s 282ms/step - loss: 0.3522 - accuracy: 0.8758 - val_loss: 0.3428 - val_accuracy: 0.8762\n",
      "Epoch 5/50\n",
      "312/312 [==============================] - 89s 284ms/step - loss: 0.3442 - accuracy: 0.8776 - val_loss: 0.3363 - val_accuracy: 0.8822\n",
      "Epoch 6/50\n",
      "312/312 [==============================] - 84s 270ms/step - loss: 0.3483 - accuracy: 0.8769 - val_loss: 0.3578 - val_accuracy: 0.8746\n",
      "Epoch 7/50\n",
      "312/312 [==============================] - 85s 272ms/step - loss: 0.3424 - accuracy: 0.8789 - val_loss: 0.3428 - val_accuracy: 0.8814\n",
      "Epoch 8/50\n",
      "312/312 [==============================] - 87s 277ms/step - loss: 0.3434 - accuracy: 0.8773 - val_loss: 0.3333 - val_accuracy: 0.8822\n",
      "Epoch 9/50\n",
      "312/312 [==============================] - 85s 271ms/step - loss: 0.3387 - accuracy: 0.8782 - val_loss: 0.3308 - val_accuracy: 0.8774\n",
      "Epoch 10/50\n",
      "312/312 [==============================] - 84s 270ms/step - loss: 0.3352 - accuracy: 0.8791 - val_loss: 0.3722 - val_accuracy: 0.8742\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train_srm, x_train_ela], y_train, batch_size=batch_size, epochs=epochs, validation_data=([x_val_srm, x_val_ela], y_val), callbacks=[early_stopping, model_checkpoint_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('two_stream.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_srm_test = [] # SRM converted images\n",
    "x_ela_test = [] # ELA converted images\n",
    "test_labels = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 images\n",
      "790 790 790\n"
     ]
    }
   ],
   "source": [
    "#place authentic\n",
    "Au_path_test = '../synthetic_test/Au'\n",
    "prepare_data(Au_path_test, 1, x_srm_test, x_ela_test, test_labels)\n",
    "# random.shuffle(X)\n",
    "print(len(x_srm_test), len(x_ela_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000 images\n",
      "1190 1190 1190\n"
     ]
    }
   ],
   "source": [
    "#place tampered\n",
    "Tp_path_test = '../synthetic_test/Tp'\n",
    "prepare_data(Tp_path_test, 0, x_srm_test, x_ela_test, test_labels)\n",
    "print(len(x_srm_test), len(x_ela_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 224, 224, 3) (1190, 224, 224, 3) (1190, 2)\n"
     ]
    }
   ],
   "source": [
    "x_test_srm, x_test2_srm, x_test_ela, x_test2_ela, y_test, y_test2 = reshaping(x_srm_test, x_ela_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "predictions = model.predict([x_test_srm, x_test_ela])\n",
    "average_precision = average_precision_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9113130895250358"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 158ms/step - loss: 0.2513 - accuracy: 0.9118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2513410449028015, 0.9117646813392639]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x_val_srm, x_val_ela], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_srm = [] # SRM converted images\n",
    "casia_ela = [] # ELA converted images\n",
    "casia_labels = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 images\n",
      "Processing 1000 images\n",
      "Processing 1500 images\n",
      "Processing 2000 images\n",
      "Processing 2500 images\n",
      "Processing 3000 images\n",
      "Processing 3500 images\n",
      "Processing 4000 images\n",
      "Processing 4500 images\n",
      "Processing 5000 images\n",
      "Processing 5500 images\n",
      "Processing 6000 images\n",
      "Processing 6500 images\n",
      "Processing 7000 images\n",
      "7329 7329 0\n",
      "7329 7329 7329\n"
     ]
    }
   ],
   "source": [
    "#place authentic\n",
    "Au_path_casia = '../forgery/data/Au'\n",
    "prepare_data(Au_path_casia, 1, casia_srm, casia_ela, casia_labels)\n",
    "# random.shuffle(X)\n",
    "print(len(casia_srm), len(casia_ela), len(casia_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7500 images\n",
      "Processing 8000 images\n",
      "Processing 8500 images\n",
      "Processing 9000 images\n",
      "9393 9393 0\n",
      "9393 9393 9393\n"
     ]
    }
   ],
   "source": [
    "#place tampered\n",
    "Tp_path_casia = '../forgery/data/Tp'\n",
    "prepare_data(Tp_path_casia, 0, casia_srm, casia_ela, casia_labels)\n",
    "print(len(casia_srm), len(casia_ela), len(casia_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9393, 224, 224, 3) (9393, 224, 224, 3) (9393, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train_casia_srm, x_val_casia_srm, x_train_casia_ela, x_val_casia_ela, y_casia_train, y_casia_val = reshaping(casia_srm, casia_ela, casia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_model = tf.keras.models.load_model('two_stream.h5')\n",
    "\n",
    "# for layer in casia_model.layers[:-1]:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(), tfa.metrics.F1Score(num_classes=2, average=\"micro\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/235 [..............................] - ETA: 28s - loss: 0.0278 - accuracy: 1.0000 - auc_1: 1.0000 - f1_score: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0789s vs `on_train_batch_end` time: 0.1641s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0789s vs `on_train_batch_end` time: 0.1641s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 68s 290ms/step - loss: 0.0711 - accuracy: 0.9687 - auc_1: 0.9969 - f1_score: 0.9687 - val_loss: 0.0888 - val_accuracy: 0.9617 - val_auc_1: 0.9946 - val_f1_score: 0.9617\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 66s 280ms/step - loss: 0.0701 - accuracy: 0.9703 - auc_1: 0.9970 - f1_score: 0.9703 - val_loss: 0.0930 - val_accuracy: 0.9638 - val_auc_1: 0.9943 - val_f1_score: 0.9638\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 63s 267ms/step - loss: 0.0698 - accuracy: 0.9697 - auc_1: 0.9971 - f1_score: 0.9697 - val_loss: 0.1046 - val_accuracy: 0.9585 - val_auc_1: 0.9934 - val_f1_score: 0.9585\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 61s 260ms/step - loss: 0.0620 - accuracy: 0.9738 - auc_1: 0.9977 - f1_score: 0.9738 - val_loss: 0.0965 - val_accuracy: 0.9585 - val_auc_1: 0.9943 - val_f1_score: 0.9585\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 61s 260ms/step - loss: 0.0660 - accuracy: 0.9707 - auc_1: 0.9970 - f1_score: 0.9707 - val_loss: 0.1268 - val_accuracy: 0.9441 - val_auc_1: 0.9910 - val_f1_score: 0.9441\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 62s 263ms/step - loss: 0.0667 - accuracy: 0.9699 - auc_1: 0.9973 - f1_score: 0.9699 - val_loss: 0.1003 - val_accuracy: 0.9574 - val_auc_1: 0.9937 - val_f1_score: 0.9574\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 63s 267ms/step - loss: 0.0680 - accuracy: 0.9709 - auc_1: 0.9972 - f1_score: 0.9709 - val_loss: 0.0998 - val_accuracy: 0.9537 - val_auc_1: 0.9939 - val_f1_score: 0.9537\n"
     ]
    }
   ],
   "source": [
    "history = casia_model.fit([x_train_casia_srm, x_train_casia_ela], y_casia_train, batch_size=batch_size, epochs=epochs, validation_data=([x_val_casia_srm, x_val_casia_ela], y_casia_val), \n",
    "                    callbacks=[early_stopping, model_checkpoint_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_model.save('casia_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "two_stream.ipynb",
   "provenance": []
  },
  "instance_type": "ml.g4dn.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
