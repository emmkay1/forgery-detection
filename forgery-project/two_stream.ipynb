{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lz8VDKved3D",
    "outputId": "182c58c2-c028-45ef-c29f-eaf1c208d4a8"
   },
   "outputs": [],
   "source": [
    "pip install matplotlib tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdJ2fWiPed3K"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization, Lambda, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input\n",
    "# import tf_slim as slim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "%matplotlib inline\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UnFDto_ed3L"
   },
   "outputs": [],
   "source": [
    "def initializer():\n",
    "    filter1 = [[0, 0, 0, 0, 0],\n",
    "               [0, -1, 2, -1, 0],\n",
    "               [0, 2, -4, 2, 0],\n",
    "               [0, -1, 2, -1, 0],\n",
    "               [0, 0, 0, 0, 0]]\n",
    "    filter2 = [[-1, 2, -2, 2, -1],\n",
    "               [2, -6, 8, -6, 2],\n",
    "               [-2, 8, -12, 8, -2],\n",
    "               [2, -6, 8, -6, 2],\n",
    "               [-1, 2, -2, 2, -1]]\n",
    "    filter3 = [[0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [0, 1, -2, 1, 0],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0]]\n",
    "    q = [4.0, 12.0, 2.0]\n",
    "    filter1 = np.asarray(filter1, dtype=float) / 4\n",
    "    filter2 = np.asarray(filter2, dtype=float) / 12\n",
    "    filter3 = np.asarray(filter3, dtype=float) / 2\n",
    "    filters = [[filter1, filter1, filter1], [filter2, filter2, filter2], [filter3, filter3, filter3]]\n",
    "    filters = np.einsum('klij->ijlk', filters)\n",
    "    filters = tf.Variable(filters, dtype=tf.float32)\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3G7HUolf9S0"
   },
   "outputs": [],
   "source": [
    "def convert_to_ela_image(path, quality):\n",
    "    temp_filename = 'temp_file_name.jpg'\n",
    "    ela_filename = 'temp_ela.png'\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image.save(temp_filename, 'JPEG', quality = quality)\n",
    "    temp_image = Image.open(temp_filename)\n",
    "    \n",
    "    ela_image = ImageChops.difference(image, temp_image)\n",
    "    \n",
    "    extrema = ela_image.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    \n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "    \n",
    "    return ela_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzNcv1sied3L"
   },
   "outputs": [],
   "source": [
    "h = 224\n",
    "w = 224\n",
    "image_size = (h, w)\n",
    "\n",
    "# np_arr = lambda img: img.resize(image_size).flatten() / 255.0\n",
    "np_arr = lambda img: np.array(img.resize(image_size)).flatten() / 255.0\n",
    "\n",
    "def prepare_image(image_path, is_ela):\n",
    "    return np_arr(convert_to_ela_image(image_path, 95)) if is_ela == 1 else np_arr(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OG4Y6Djyed3L"
   },
   "outputs": [],
   "source": [
    "x_srm = [] # SRM converted images\n",
    "x_ela = [] # ELA converted images\n",
    "labels = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZnqIkRred3M"
   },
   "outputs": [],
   "source": [
    "def prepare_data(path, cls, srm, ela, targets):\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('jpg') or filename.endswith('png'):\n",
    "                try:\n",
    "                    full_path = os.path.join(dirname, filename)\n",
    "                    srm.append(prepare_image(full_path, 0))\n",
    "                    ela.append(prepare_image(full_path, 1))\n",
    "                    targets.append(cls)\n",
    "                except:\n",
    "                    pass\n",
    "                if len(targets) % 500 == 0:\n",
    "                    print('Processing {} images'.format(len(targets)))\n",
    "    print(len(srm), len(ela), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nv-AJSfed3M",
    "outputId": "555d4c5a-b5e2-43a7-e861-87b2ce4a3de7"
   },
   "outputs": [],
   "source": [
    "#place authentic\n",
    "Au_path = '../synthetic/Au'\n",
    "prepare_data(Au_path, 1, x_srm, x_ela, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCDWFw6Oed3M",
    "outputId": "e35933dd-1ce3-4549-a001-8c8b29373a7e"
   },
   "outputs": [],
   "source": [
    "#place tampered\n",
    "Tp_path = '../synthetic/Tp'\n",
    "prepare_data(Tp_path, 0, x_srm, x_ela, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaping(srm, ela, targets):\n",
    "    srm = np.array(srm)\n",
    "    ela = np.array(ela)\n",
    "\n",
    "    targets = to_categorical(targets, 2)\n",
    "\n",
    "    srm = srm.reshape(-1, h, w, 3)\n",
    "    ela = ela.reshape(-1, h, w, 3)\n",
    "\n",
    "    print(srm.shape, ela.shape, targets.shape)\n",
    "    \n",
    "    # stack so we can split on the same pair of images\n",
    "    x_combined = np.stack((srm, ela), axis=4)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_combined, targets, test_size = 0.2, random_state=5)\n",
    "\n",
    "    # take them apart\n",
    "    x_train_srm = x_train[:,:,:,:,0]\n",
    "    x_val_srm = x_val[:,:,:,:,0]\n",
    "\n",
    "    x_train_ela = x_train[:,:,:,:,1]\n",
    "    x_val_ela = x_val[:,:,:,:,1]\n",
    "    \n",
    "    return x_train_srm, x_val_srm, x_train_ela, x_val_ela, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_srm, x_val_srm, x_train_ela, x_val_ela, y_train, y_val = reshaping(x_srm, x_ela, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyqdzWMGed3N"
   },
   "outputs": [],
   "source": [
    "initializer_srm = initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_product(x):\n",
    "    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
    "    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n",
    "    \n",
    "    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
    "    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n",
    "    \n",
    "    # Divide by feature map size [sizexsize]\n",
    "    size1 = int(x[1].shape[1])\n",
    "    size2 = int(x[1].shape[2])\n",
    "    phi_I = tf.divide(phi_I, size1*size2)\n",
    "    \n",
    "    # Take signed square root of phi_I\n",
    "    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n",
    "    \n",
    "    # Apply l2 normalization\n",
    "    z_l2 = tf.math.l2_normalize(y_ssqrt, axis=1)\n",
    "    return z_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7iZvzfEkuIx"
   },
   "outputs": [],
   "source": [
    "def conv_layers(input):\n",
    "    x = Conv2D(32, 3, padding='valid', activation='relu')(input)\n",
    "    x = Conv2D(32, 3, padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(64, 3, padding='valid', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, 3, padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, 5, padding='valid', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 5, padding='valid', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "#     x = Conv2D(256, 5, padding='valid', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Conv2D(256, 5, padding='valid', activation='relu')(x)\n",
    "#     x = MaxPool2D(pool_size=2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "#   x = Conv2D(512, 3, padding='valid', activation='relu')(x)\n",
    "#   x = BatchNormalization()(x)\n",
    "#   x = Conv2D(512, 3, padding='valid', activation='relu')(x)\n",
    "#   x = MaxPool2D(pool_size=2)(x)\n",
    "#   model = BatchNormalization()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layers_2(input):\n",
    "    x = Conv2D(32, 3, padding='same')(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(32, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(64, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(256, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, 3, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ptBj_9MkVvF"
   },
   "outputs": [],
   "source": [
    "# SRM stream\n",
    "srm_input = Input(shape=[h, w, 3], name='srm_input')\n",
    "op = tf.nn.conv2d(srm_input, initializer_srm, strides=[1, 1, 1, 1], padding='SAME', name='srm-layer')\n",
    "srm_model = conv_layers(op)\n",
    "\n",
    "# ELA stream\n",
    "ela_input = Input(shape=[h, w, 3], name='ela_input')\n",
    "ela_model = conv_layers(ela_input)\n",
    "\n",
    "# Concatenate streams\n",
    "# concat = tf.keras.layers.Concatenate()([srm_model, ela_model])\n",
    "\n",
    "# Bilinear fusion\n",
    "x = Lambda(outer_product, name='outer_product')([srm_model,ela_model])\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "# x = Dense(256)(x)\n",
    "# x = LeakyReLU(alpha=0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[srm_input, ela_input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV1_nNvued3O"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "init_lr = 1e-4\n",
    "# optimizer = Adam(lr = init_lr)\n",
    "optimizer = Adam(learning_rate = init_lr, amsgrad=True)\n",
    "# decay = init_lr/epochs\n",
    "# optimizer = RMSprop(learning_rate = init_lr, centered = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0,patience=5, verbose=0, mode='auto')\n",
    "\n",
    "checkpoint_filepath = 'two_stream/checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "# model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOn-5ptued3O",
    "outputId": "f3b32633-a783-4093-f716-8309c7e5fdc0"
   },
   "outputs": [],
   "source": [
    "history = model.fit([x_train_srm, x_train_ela], y_train, batch_size=batch_size, epochs=epochs, validation_data=([x_val_srm, x_val_ela], y_val), callbacks=[early_stopping, model_checkpoint_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('two_stream.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_srm_test = [] # SRM converted images\n",
    "x_ela_test = [] # ELA converted images\n",
    "test_labels = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place authentic\n",
    "Au_path_test = '../synthetic_test/Au'\n",
    "prepare_data(Au_path_test, 1, x_srm_test, x_ela_test, test_labels)\n",
    "# random.shuffle(X)\n",
    "print(len(x_srm_test), len(x_ela_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place tampered\n",
    "Tp_path_test = '../synthetic_test/Tp'\n",
    "prepare_data(Tp_path_test, 0, x_srm_test, x_ela_test, test_labels)\n",
    "print(len(x_srm_test), len(x_ela_test), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_srm, x_test2_srm, x_test_ela, x_test2_ela, y_test, y_test2 = reshaping(x_srm_test, x_ela_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "predictions = model.predict([x_test_srm, x_test_ela])\n",
    "average_precision = average_precision_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_val_srm, x_val_ela], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_srm = [] # SRM converted images\n",
    "casia_ela = [] # ELA converted images\n",
    "casia_labels = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place authentic\n",
    "Au_path_casia = '../forgery/data/Au'\n",
    "prepare_data(Au_path_casia, 1, casia_srm, casia_ela, casia_labels)\n",
    "# random.shuffle(X)\n",
    "print(len(casia_srm), len(casia_ela), len(casia_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place tampered\n",
    "Tp_path_casia = '../forgery/data/Tp'\n",
    "prepare_data(Tp_path_casia, 0, casia_srm, casia_ela, casia_labels)\n",
    "print(len(casia_srm), len(casia_ela), len(casia_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_casia_srm, x_val_casia_srm, x_train_casia_ela, x_val_casia_ela, y_casia_train, y_casia_val = reshaping(casia_srm, casia_ela, casia_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_model = tf.keras.models.load_model('two_stream.h5')\n",
    "\n",
    "# for layer in casia_model.layers[:-1]:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(), tfa.metrics.F1Score(num_classes=2, average=\"micro\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = casia_model.fit([x_train_casia_srm, x_train_casia_ela], y_casia_train, batch_size=batch_size, epochs=epochs, validation_data=([x_val_casia_srm, x_val_casia_ela], y_casia_val), \n",
    "                    callbacks=[early_stopping, model_checkpoint_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_model.save('casia_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "two_stream.ipynb",
   "provenance": []
  },
  "instance_type": "ml.g4dn.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}